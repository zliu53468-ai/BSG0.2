# -*- coding: utf-8 -*-
import numpy as np
import joblib
import os
import random
import warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
from hmmlearn import hmm

# å¿½ç•¥ HMM learn çš„ä¸€äº›èˆŠç‰ˆæœ¬è­¦å‘Š
warnings.filterwarnings("ignore", category=DeprecationWarning) 

# =============================================================================
# å…¨åŸŸè¨­å®š
# =============================================================================
MODEL_DIR = 'models'
SYNTHETIC_DATA_SIZE = 5000 # ç”Ÿæˆ5000ç­†æ¨¡æ“¬æ•¸æ“šä»¥é€²è¡Œç©©å¥çš„è¨“ç·´
N_FEATURES_WINDOW = 20     # æå–ç‰¹å¾µæ™‚å›çœ‹çš„æ™‚é–“çª—å£å¤§å°
LABEL_MAP = {'B': 0, 'P': 1}

# =============================================================================
# ç‰¹å¾µæå–å‡½å¼ (æ­¤å‡½å¼å¿…é ˆèˆ‡ app.py ä¸­çš„ç‰ˆæœ¬é‚è¼¯ä¸€è‡´)
# =============================================================================
def extract_features_for_training(full_roadmap):
    """å¾å®Œæ•´çš„è·¯å–®ä¸­ç‚ºè¨“ç·´æå–ç‰¹å¾µå’Œæ¨™ç±¤ã€‚"""
    features_list = []
    labels = []
    
    # éœ€è¦è‡³å°‘ N+1 å€‹æ•¸æ“šé»æ‰èƒ½æå–ç¬¬ä¸€çµ„ç‰¹å¾µåŠå…¶æ¨™ç±¤
    if len(full_roadmap) <= N_FEATURES_WINDOW:
        return np.array([]), np.array([])

    for i in range(N_FEATURES_WINDOW, len(full_roadmap)):
        window = full_roadmap[i-N_FEATURES_WINDOW:i]
        label = full_roadmap[i]

        # åªç‚ºæœ‰æ˜ç¢ºæ¨™ç±¤ (B/P) çš„æ•¸æ“šé»å‰µå»ºè¨“ç·´æ¨£æœ¬
        if label not in LABEL_MAP:
            continue

        b_count = window.count('B')
        p_count = window.count('P')
        total = b_count + p_count

        b_ratio = b_count / total if total > 0 else 0.5
        p_ratio = p_count / total if total > 0 else 0.5
        
        streak = 0
        last_result = None
        for item in reversed(window):
            if item in ['B', 'P']:
                if last_result is None:
                    last_result = item
                    streak = 1
                elif item == last_result:
                    streak += 1
                else:
                    break
        
        streak_type = LABEL_MAP.get(last_result, -1)
        prev_result = LABEL_MAP.get(window[-1], -1) if window else -1

        features = [b_ratio, p_ratio, streak, streak_type, prev_result]
        features_list.append(features)
        labels.append(LABEL_MAP[label])

    return np.array(features_list), np.array(labels)

# =============================================================================
# ä¸»è¦è¨“ç·´é‚è¼¯
# =============================================================================
def train():
    """åŸ·è¡Œå®Œæ•´çš„æ¨¡å‹è¨“ç·´æµç¨‹ã€‚"""
    print("="*50)
    print("é–‹å§‹é‡æ–°è¨“ç·´ AI æ¨¡å‹...")
    print("="*50)

    # 1. å»ºç«‹æ¨¡å‹å„²å­˜ç›®éŒ„
    if not os.path.exists(MODEL_DIR):
        os.makedirs(MODEL_DIR)
        print(f"âœ… å·²å»ºç«‹ç›®éŒ„: {MODEL_DIR}")

    # 2. ç”Ÿæˆå¹³è¡¡çš„æ¨¡æ“¬æ•¸æ“šä»¥é¿å…åå·®
    print(f"ğŸ”„ æ­£åœ¨ç”Ÿæˆ {SYNTHETIC_DATA_SIZE} ç­†é«˜å“è³ªæ¨¡æ“¬æ•¸æ“š...")
    synthetic_roadmap = []
    # æ ¹æ“šç™¾å®¶æ¨‚çœŸå¯¦æ©Ÿç‡ (æ’é™¤å’Œå±€å¾Œ): é–’å®¶å‹ç‡ç´„ 49.32%, èŠå®¶å‹ç‡ç´„ 50.68%
    p_win_prob = 0.4932 
    for _ in range(SYNTHETIC_DATA_SIZE):
        if random.random() < p_win_prob:
            synthetic_roadmap.append('P')
        else:
            synthetic_roadmap.append('B')
    print("âœ… æ¨¡æ“¬æ•¸æ“šç”Ÿæˆå®Œç•¢ã€‚")

    # 3. æå–åŸºç¤ç‰¹å¾µ
    print("ğŸ”„ æ­£åœ¨æå–åŸºç¤ç‰¹å¾µ...")
    X_basic, y = extract_features_for_training(synthetic_roadmap)

    if len(X_basic) == 0:
        print("âŒ éŒ¯èª¤ï¼šç„¡æ³•å¾æ•¸æ“šä¸­æå–ä»»ä½•ç‰¹å¾µã€‚è¨“ç·´ä¸­æ­¢ã€‚")
        return
    print(f"âœ… åŸºç¤ç‰¹å¾µæå–å®Œæˆï¼Œå…± {len(y)} ç­†è¨“ç·´æ¨£æœ¬ã€‚")

    # 4. åœ¨å®Œæ•´çš„åºåˆ—ä¸Šè¨“ç·´ HMM æ¨¡å‹
    print("ğŸ”„ æ­£åœ¨è¨“ç·´ HMM æ¨¡å‹...")
    hmm_roadmap_numeric = np.array([LABEL_MAP[r] for r in synthetic_roadmap if r in LABEL_MAP]).reshape(-1, 1)
    
    # ã€é—œéµä¿®æ­£ã€‘: å°æ–¼é›¢æ•£è§€æ¸¬å€¼ (èŠ/é–’)ï¼Œå¿…é ˆä½¿ç”¨ CategoricalHMM
    # n_components æ˜¯éš±è—ç‹€æ…‹çš„æ•¸é‡ï¼Œ4æ˜¯ä¸€å€‹åˆç†çš„èµ·å§‹å€¼
    hmm_model = hmm.CategoricalHMM(n_components=4, n_iter=100, random_state=42, tol=0.01)
    hmm_model.fit(hmm_roadmap_numeric)
    joblib.dump(hmm_model, os.path.join(MODEL_DIR, 'hmm_model.pkl'))
    print("âœ… HMM æ¨¡å‹ (hmm_model.pkl) å·²è¨“ç·´ä¸¦å„²å­˜ã€‚")

    # 5. ä½¿ç”¨è¨“ç·´å¥½çš„ HMM æ¨¡å‹æå–é€²éšç‰¹å¾µ
    print("ğŸ”„ æ­£åœ¨ä½¿ç”¨ HMM æ¨¡å‹æå–é€²éšç‰¹å¾µ...")
    hmm_features = []
    for i in range(N_FEATURES_WINDOW, len(synthetic_roadmap)):
        if synthetic_roadmap[i] not in LABEL_MAP:
            continue

        current_roadmap_numeric = np.array([LABEL_MAP[r] for r in synthetic_roadmap[:i] if r in LABEL_MAP]).reshape(-1, 1)
        
        if len(current_roadmap_numeric) < 1:
            hmm_features.append([0.5, 0.5]) # å°æ–¼å¤ªçŸ­çš„åºåˆ—ä½¿ç”¨é è¨­æ©Ÿç‡
            continue
            
        try:
            # ä½¿ç”¨èˆ‡ app.py ç›¸åŒçš„ç©©å¥é æ¸¬é‚è¼¯
            hidden_states = hmm_model.predict(current_roadmap_numeric)
            last_state = hidden_states[-1]
            transition_probs = hmm_model.transmat_[last_state, :]
            emission_probs = hmm_model.emissionprob_
            
            prob_b = np.dot(transition_probs, emission_probs[:, LABEL_MAP['B']])
            prob_p = np.dot(transition_probs, emission_probs[:, LABEL_MAP['P']])
            
            total_prob = prob_b + prob_p
            if total_prob > 1e-9:
                hmm_features.append([prob_b/total_prob, prob_p/total_prob])
            else:
                hmm_features.append([0.5, 0.5])
        except Exception:
            hmm_features.append([0.5, 0.5]) # å¦‚æœå‡ºéŒ¯å‰‡ä½¿ç”¨é è¨­å€¼
    
    X_combined = np.concatenate([X_basic, np.array(hmm_features)], axis=1)
    print(f"âœ… é€²éšç‰¹å¾µæå–å®Œæˆï¼Œæœ€çµ‚ç‰¹å¾µç¶­åº¦: {X_combined.shape[1]}")

    # 6. è¨“ç·´ Scaler å’Œ XGBoost
    X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42, stratify=y)
    
    print("ğŸ”„ æ­£åœ¨æ¨™æº–åŒ–ç‰¹å¾µ (Scaler)...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    joblib.dump(scaler, os.path.join(MODEL_DIR, 'scaler.pkl'))
    print("âœ… æ¨™æº–åŒ–å™¨ (scaler.pkl) å·²å„²å­˜ã€‚")
    
    print("ğŸ”„ æ­£åœ¨è¨“ç·´ XGBoost æœ€çµ‚æ¨¡å‹...")
    xgb_model = XGBClassifier(
        objective='binary:logistic', eval_metric='logloss',
        n_estimators=150, learning_rate=0.05, max_depth=4,
        use_label_encoder=False, random_state=42
    )
    xgb_model.fit(X_train_scaled, y_train)
    
    accuracy = xgb_model.score(X_test_scaled, y_test)
    print(f"ğŸ“ˆ æ¨¡å‹åœ¨æ¸¬è©¦é›†ä¸Šçš„æº–ç¢ºç‡: {accuracy:.4f}")
    
    joblib.dump(xgb_model, os.path.join(MODEL_DIR, 'xgb_model.pkl'))
    print("âœ… XGBoost æ¨¡å‹ (xgb_model.pkl) å·²å„²å­˜ã€‚")
    
    # 7. å„²å­˜ç‰¹å¾µè³‡è¨Š
    feature_info = {'use_hmm_features': True}
    joblib.dump(feature_info, os.path.join(MODEL_DIR, 'feature_info.pkl'))
    print("âœ… ç‰¹å¾µè³‡è¨Š (feature_info.pkl) å·²å„²å­˜ã€‚")

    print("\nğŸ‰ æ‰€æœ‰æ¨¡å‹å·²æˆåŠŸé‡æ–°è¨“ç·´ä¸¦å„²å­˜ï¼")

if __name__ == '__main__':
    train()

